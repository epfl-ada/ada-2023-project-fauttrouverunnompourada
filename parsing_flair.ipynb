{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary parsing using Flair & Classification of our characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to:\n",
    "- cycle through all summaries in the `plot_summaries.txt` file and count the number of occurences of each character name inside\n",
    "- use this data to classify the characters present inside of the `character.metadata.tsv` file into three categories:\n",
    "    - **Primary:** the character name takes up over 10% of all mentioned characters\n",
    "    - **Secondary:** the character name takes up less than 10% of all mentioned characters\n",
    "    - **Missed:** the character name is not mentioned in the movie summary at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import missingno as msno\n",
    "from geopy.geocoders import Nominatim\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import pycountry_convert as pc\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import dataframes as RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.nn import Classifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "# Load the model\n",
    "tagger = Classifier.load('ner-fast')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `extract_character_names_flair` takes a string (which can be one of the summaries) and goes through all the words inside. If the word is considered a name, then it is appended to a list. The list of names is returned at the end of the function.\n",
    "\n",
    "The function `count_appearances` takes a large string (it can be the summary) and a list of strings (it can be the list of characters found with the first function), and counts the number of times all strings in the list appear in the large text. The function returns a dictionary with the strings from the list and their occurence count inside of the large text.\n",
    "\n",
    "These two functions will be used in the following way for all summaries:\n",
    "- Use `extract_character_names_flair` to identify all character names inside\n",
    "- Use `count_appearances` to count the number of appearances of all character names in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_character_names_flair(summary):\n",
    "    # Create a Flair Sentence\n",
    "    sentence = Sentence(summary)\n",
    "\n",
    "    # Run NER on the sentence\n",
    "    tagger.predict(sentence)\n",
    "\n",
    "    # Extract character names (NER tags labeled as PER, indicating a person)\n",
    "    character_names = []\n",
    "\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        if entity.tag == 'PER':\n",
    "            character_names.append(entity.text)\n",
    "\n",
    "    return character_names\n",
    "\n",
    "def count_appearances(larger_string, string_list):\n",
    "    # Initialize an empty dictionary to store counts\n",
    "    appearances_dict = {}\n",
    "\n",
    "    # Iterate over each string in the list\n",
    "    for search_string in string_list:\n",
    "        # Count occurrences using the count() method (we convert to lowercase to avoid missing any occurence)\n",
    "        count = larger_string.lower().count(search_string.lower())\n",
    "        \n",
    "        # Store the count in the dictionary\n",
    "        appearances_dict[search_string] = count\n",
    "\n",
    "    return appearances_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us import the summary data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = RAW.summaries.copy()\n",
    "summaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the two functions mentionned above to cycle through all summaries and create the character appearance dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a subset of the data (it takes about 8h for 10,000 summaries)\n",
    "sub_summaries = summaries.iloc[15000:20000, :].copy()\n",
    "\n",
    "parsing_results = []\n",
    "\n",
    "for index, row in sub_summaries.iterrows():\n",
    "    # Print the index to keep track of where we are in the parsing\n",
    "    print(index)\n",
    "\n",
    "    # Extract the names from the summary\n",
    "    names = set(extract_character_names_flair(row['Summary']))\n",
    "\n",
    "    # Count the appearances of every name\n",
    "    counts = count_appearances(row['Summary'], names)\n",
    "\n",
    "    # Append the dictionary to the result list\n",
    "    parsing_results.append(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list `parsing_results` will now contain all the dictionaries from the character counting. In order to have a better wiew of the distribution of the characters in each movie summary, we can rank the dictionaries in descending order (to get the most common names at the beginning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsing_results = [\n",
    "    {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "    for d in parsing_results\n",
    "]\n",
    "\n",
    "sub_summaries['Characters'] = parsing_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the resulting dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_summaries.to_csv('parsing_15000_19999.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the goal is to classify all characters from the character data into three roles: Primary, Secondary and Missed (explained earlier). First, let us import the data and add a `Role` column (filled with NaN values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = RAW.character_data.copy()\n",
    "characters['Role'] = np.nan\n",
    "characters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will classify the characters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First version: characters who take up over 10% of all names are primary and the rest are secondary (many characters are classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "for index, row in sub_summaries.iterrows():\n",
    "    print(index)\n",
    "\n",
    "    # Wiki ID of the movie to consider\n",
    "    wiki_id = row['Wiki ID']\n",
    "\n",
    "    # Dictionary of the parsing results for this movie\n",
    "    parsing_result = row['Characters']\n",
    "\n",
    "    # All characters who belong to this movie\n",
    "    sub_characters = characters[characters['Wiki ID'] == wiki_id]\n",
    "    \n",
    "    # If the movie features actors inside of the character dataframe then proceed\n",
    "    if not(sub_characters.empty):\n",
    "        for i, r in sub_characters.iterrows():\n",
    "            # Take one of the characters\n",
    "            character = r['Character name']\n",
    "\n",
    "            # If the considered character has a valid name then proceed\n",
    "            if not(pd.isna(character)):\n",
    "                # Split the character in all of its words (name, surname, etc)\n",
    "                split_character_name = character.split()\n",
    "\n",
    "                count = 0\n",
    "                total = 0\n",
    "\n",
    "                for key, value in parsing_result.items():\n",
    "                    # Add all values to the total\n",
    "                    total += value\n",
    "\n",
    "                    for item in split_character_name:\n",
    "                        if item in key:\n",
    "                            # If we find a match then add to the count and stop (to avoid counting twice)\n",
    "                            count += value\n",
    "                            break\n",
    "                    \n",
    "                if total != 0:\n",
    "                    # Compute ratio\n",
    "                    ratio = count / total\n",
    "                else:\n",
    "                    # Empty dictionary: the character is a miss\n",
    "                    ratio = 0\n",
    "\n",
    "                if ratio > 0.1:\n",
    "                    # Primary character: appears 10% of the time or more\n",
    "                    characters.loc[(characters['Character name'] == character) & (characters['Wiki ID'] == wiki_id), 'Role'] = 'Primary'\n",
    "\n",
    "                elif ratio <= 0.1 and ratio > 0:\n",
    "                    # Secondary character: appears less than 10%\n",
    "                    characters.loc[(characters['Character name'] == character) & (characters['Wiki ID'] == wiki_id), 'Role'] = 'Secondary'\n",
    "\n",
    "                else:\n",
    "                    # None: The character was not mentioned in the summary\n",
    "                    characters.loc[(characters['Character name'] == character) & (characters['Wiki ID'] == wiki_id), 'Role'] = 'Missed'\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Second version: the most common character is primary and the second most is secondary (only 2 characters are classified and there are no 'Missed' category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in sub_summaries.iterrows():\n",
    "    print(index)\n",
    "\n",
    "    # Wiki ID of the movie to consider\n",
    "    wiki_id = row['Wiki ID']\n",
    "\n",
    "    # Dictionary of the parsing results for this movie\n",
    "    parsing_result = row['Characters']\n",
    "    parsing_result = ast.literal_eval(parsing_result)\n",
    "\n",
    "    # All characters who belong to this movie\n",
    "    sub_characters = characters[characters['Wiki ID'] == wiki_id]\n",
    "\n",
    "    primary = None\n",
    "    secondary = None\n",
    "\n",
    "    most = -1\n",
    "    second_most = -1\n",
    "    \n",
    "    # If the movie features actors inside of the character dataframe then proceed\n",
    "    if not(sub_characters.empty):\n",
    "        for i, r in sub_characters.iterrows():\n",
    "            # Take one of the characters\n",
    "            character = r['Character name']\n",
    "            char_index = i\n",
    "\n",
    "            # If the considered character has a valid name then proceed\n",
    "            if not(pd.isna(character)):\n",
    "                # Split the character in all of its words (name, surname, etc)\n",
    "                split_character_name = character.split()\n",
    "\n",
    "                count = 0\n",
    "                total = 0\n",
    "\n",
    "                for key, value in parsing_result.items():\n",
    "                    # Add all values to the total\n",
    "                    total += value\n",
    "\n",
    "                    for item in split_character_name:\n",
    "                        if item in key:\n",
    "                            # If we find a match then add to the count and stop (to avoid counting twice)\n",
    "                            count += value\n",
    "                            break\n",
    "                    \n",
    "                if total != 0:\n",
    "                    # Compute ratio\n",
    "                    ratio = count / total\n",
    "                else:\n",
    "                    # Empty dictionary: the character is a miss\n",
    "                    ratio = 0\n",
    "\n",
    "                # Found a new character that appears more often than the current first\n",
    "                if ratio > most:\n",
    "                    # Current first becomes second\n",
    "                    second_most = most\n",
    "                    secondary = primary\n",
    "\n",
    "                    # New character gets first place\n",
    "                    most = ratio\n",
    "                    primary = i\n",
    "\n",
    "                else:\n",
    "                    # Found a new character that appears as often as the current first and there are still no second most\n",
    "                    if ratio == most and secondary == None:\n",
    "                        second_most = ratio\n",
    "                        secondary = i\n",
    "\n",
    "                    # Found a new character that appears less often than the current first and more often than the current second\n",
    "                    if ratio < most and ratio > second_most:\n",
    "                        second_most = ratio\n",
    "                        secondary = i\n",
    "\n",
    "        # If we couldn't classify both a primary and a secondary character then the movie is not useful\n",
    "        if primary != None and secondary != None:\n",
    "            characters.at[primary, characters.columns[-1]] = 'Primary'\n",
    "            characters.at[secondary, characters.columns[-1]] = 'Secondary'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characters who still have a NaN value inside of their `Role` column are characters who are not featured inside of their summaries, so they will not be useful. Therefore, we filter the characters who were assigned a role:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = characters[characters['Role'].notna()]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we store the result inside of a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('character_classification_15000_19999.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
